{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel Introduction - Enhanced Edition\n",
    "\n",
    "This notebook provides an introduction to Microsoft Semantic Kernel, a powerful framework for building AI-powered applications. We'll explore the core concepts, compare different approaches, and understand how Semantic Kernel simplifies AI integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's load our configuration settings. This includes API keys, endpoints, and model configurations that we'll use throughout this notebook.\n",
    "\n",
    "If you hadn't done so already, run the notebook [0-AI-settings](./0-AI-settings.ipynb) which will collect the necessary settings in an interactive way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "- Model: gpt-4o\n",
      "- Endpoint: https://eastus.api.cognitive.microsoft.com/\n",
      "- API Key configured: True\n"
     ]
    }
   ],
   "source": [
    "// Load helper functions and configuration settings\n",
    "// This imports utility functions for loading API keys, endpoints, and other settings from settings.json\n",
    "#!import config/Settings.cs \n",
    "\n",
    "// Load configuration from settings.json file\n",
    "// This returns a tuple with all the necessary configuration values\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId, embeddingEndpoint, embeddingApiKey) = Settings.LoadFromFile();\n",
    "\n",
    "Console.WriteLine($\"Configuration loaded:\");\n",
    "Console.WriteLine($\"- Model: {model}\");\n",
    "Console.WriteLine($\"- Endpoint: {azureEndpoint}\");\n",
    "Console.WriteLine($\"- API Key configured: {!string.IsNullOrEmpty(apiKey)}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Direct AI API Calls with Microsoft.Extensions.AI\n",
    "\n",
    "Before diving into Semantic Kernel, let's see how you would typically make direct calls to AI services using Microsoft.Extensions.AI. This will help us understand what Semantic Kernel abstracts away and why it's valuable.\n",
    "\n",
    "### Microsoft.Extensions.AI Overview\n",
    "\n",
    "Microsoft.Extensions.AI is a unified abstraction layer for AI services that provides:\n",
    "- Consistent interfaces across different AI providers\n",
    "- Built-in logging, telemetry, and caching\n",
    "- Dependency injection integration\n",
    "- Rate limiting and retry policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 2.2.0-beta.5</span></li><li><span>Azure.Core, 1.47.0</span></li><li><span>Microsoft.Extensions.AI, 9.7.1</span></li><li><span>Microsoft.Extensions.AI.OpenAI, 9.7.1-preview.1.25365.4</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft.Extensions.AI packages loaded successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Import required NuGet packages for Microsoft.Extensions.AI\n",
    "#r \"nuget: Azure.Core, 1.47.0\"\n",
    "#r \"nuget: Microsoft.Extensions.AI, 9.7.1\"\n",
    "#r \"nuget: Azure.AI.OpenAI,  2.2.0-beta.5\"\n",
    "#r \"nuget: Microsoft.Extensions.AI.OpenAI, 9.7.1-preview.1.25365.4\"\n",
    "\n",
    "Console.WriteLine(\"Microsoft.Extensions.AI packages loaded successfully.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct API Response:\n",
      "====================\n",
      "To identify the Kubernetes pod that consumes the most resources, you can use tools and commands to query resource usage metrics from your cluster. Here are the most common methods:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Using `kubectl top` Command**\n",
      "\n",
      "The `kubectl top` command shows resource consumption metrics (CPU and memory) for pods directly. You'll need `metrics-server` deployed in your cluster for this command to work. If not already installed, follow the [metrics-server installation guide](https://github.com/kubernetes-sigs/metrics-server).\n",
      "\n",
      "#### Command to Check All Pods in a Namespace\n",
      "```bash\n",
      "kubectl top pods -n <namespace>\n",
      "```\n",
      "\n",
      "Example Output:\n",
      "```\n",
      "NAME                     CPU(cores)   MEMORY(bytes)\n",
      "my-app-57c8bdc89-m2k46   500m         200Mi\n",
      "my-app-57c8bdc89-n2g45   700m         300Mi\n",
      "```\n",
      "\n",
      "#### To Find the Pod Using the Most CPU or Memory\n",
      "You can sort the output using tools like `sort` to find the maximum CPU or memory usage.\n",
      "\n",
      "**Sort by CPU usage:**\n",
      "```bash\n",
      "kubectl top pods -n <namespace> --no-headers | sort -k2 -nr | head -n 1\n",
      "```\n",
      "\n",
      "**Sort by Memory usage:**\n",
      "```bash\n",
      "kubectl top pods -n <namespace> --no-headers | sort -k3 -nr | head -n 1\n",
      "```\n",
      "\n",
      "The above commands will show the pod consuming the most CPU or memory in the specified namespace.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Using `kubectl describe pod`**\n",
      "\n",
      "If you're troubleshooting resource limits, you can review individual pod resource usage using `kubectl describe` and check the `Resource Usage` or `limit` & `request` fields.\n",
      "\n",
      "#### Example:\n",
      "```bash\n",
      "kubectl describe pod <pod-name> -n <namespace>\n",
      "```\n",
      "Look at the `Limits`, `Requests`, and `Usage` fields to see how much CPU/memory is being used.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **Using Resource Metrics API**\n",
      "\n",
      "Query the `metrics.k8s.io` API directly if installed. You can run:\n",
      "```bash\n",
      "kubectl get --raw \"/apis/metrics.k8s.io/v1beta1/namespaces/<namespace>/pods\" | jq\n",
      "```\n",
      "\n",
      "This command fetches pod usage metrics in JSON format. Use tools like `jq` to process the output and find the pod with the highest resource usage.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **Using Monitoring Tools**\n",
      "If you have a monitoring system such as Prometheus, Grafana, Datadog, or Cloud provider monitoring (e.g., AWS CloudWatch, GCP Operations) integrated into your cluster:\n",
      "- Query your metrics for resource usage.\n",
      "- Use their visualization tools to find the pod with the highest resource usage.\n",
      "\n",
      "For Prometheus, you can use queries like:\n",
      "\n",
      "**CPU Usage Query:**\n",
      "```promql\n",
      "sum(rate(container_cpu_usage_seconds_total{namespace=\"<namespace>\", pod!=\"\"}[5m])) by (pod)\n",
      "```\n",
      "\n",
      "**Memory Usage Query:**\n",
      "```promql\n",
      "sum(container_memory_working_set_bytes{namespace=\"<namespace>\", pod!=\"\"}) by (pod)\n",
      "```\n",
      "\n",
      "Sort the results to find the pod using the most resources.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **Using `kubectl stats` (alternative plugins)**\n",
      "\n",
      "You can also explore additional `kubectl` plugins like [`kubectl-resource-capacity`](https://github.com/robscott/kubectl-resource-capacity) or [`kubectl-df-pv`](https://github.com/yashbhutwala/kubectl-df-pv) to get insights into resource usage.\n",
      "\n",
      "---\n",
      "\n",
      "### Troubleshooting:\n",
      "- Ensure that `metrics-server` is installed and running correctly. Without `metrics-server`, you cannot use `kubectl top` or fetch live resource usage metrics.\n",
      "- If your workloads are throttling or hitting resource limits, consider adjusting resource requests/limits in pod specs.\n",
      "\n",
      "Let me know if you need more clarification!\n"
     ]
    }
   ],
   "source": [
    "// Create a chat client using Microsoft.Extensions.AI\n",
    "\n",
    "using System.ClientModel;\n",
    "using Microsoft.Extensions.AI; \n",
    "using Azure.AI.OpenAI;\n",
    "\n",
    "IChatClient chatClient = new AzureOpenAIClient(\n",
    "        new Uri(azureEndpoint),           // Azure OpenAI endpoint URL\n",
    "        new ApiKeyCredential(apiKey))     // API key for authentication\n",
    "    .GetChatClient(model)                // Get chat client for specific model\n",
    "    .AsIChatClient();                    // Convert to Microsoft.Extensions.AI interface\n",
    "\n",
    "// Make a direct API call to the AI service\n",
    "// This is a simple request-response pattern\n",
    "ChatResponse response = await chatClient.GetResponseAsync(\n",
    "    new ChatMessage(ChatRole.User, \"how do i get the k8s pod that consume the most resources\"));\n",
    "\n",
    "Console.WriteLine(\"Direct API Response:\");\n",
    "Console.WriteLine(\"====================\");\n",
    "Console.WriteLine(response.Text);\n",
    "\n",
    "// Note: With direct API calls, you need to:\n",
    "// 1. Manage authentication and endpoints manually\n",
    "// 2. Handle conversation history yourself\n",
    "// 3. Implement prompt templating from scratch\n",
    "// 4. Build your own plugin/function calling system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using Semantic Kernel\n",
    "\n",
    "Now let's see how Semantic Kernel simplifies AI integration and provides additional capabilities.\n",
    "\n",
    "### Why Semantic Kernel?\n",
    "\n",
    "While direct API calls work, Semantic Kernel provides several advantages:\n",
    "\n",
    "1. **Higher-level abstractions**: Focus on your business logic, not AI plumbing\n",
    "2. **Built-in prompt templating**: Powerful template system with variable substitution\n",
    "3. **Plugin system**: Easy integration of custom functions and external APIs\n",
    "4. **Planning capabilities**: Automatic orchestration of complex AI workflows\n",
    "5. **Memory management**: Built-in conversation history and context management\n",
    "6. **Enterprise features**: Logging, telemetry, security, and scalability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.61.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel package loaded successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Import Semantic Kernel\n",
    "#r \"nuget: Microsoft.SemanticKernel, 1.61.0\"\n",
    "using Microsoft.SemanticKernel;\n",
    "using Kernel = Microsoft.SemanticKernel.Kernel;  // Alias to avoid conflicts\n",
    "\n",
    "Console.WriteLine(\"Semantic Kernel package loaded successfully.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Configuring a Kernel\n",
    "\n",
    "The `Kernel` is the central orchestrator in Semantic Kernel. Think of it as the \"brain\" that coordinates all AI operations, manages services, and executes prompts and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel created and configured successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Create a Kernel builder - this is the factory for creating Kernel instances\n",
    "IKernelBuilder kernelBuilder = Kernel.CreateBuilder();\n",
    "\n",
    "// Add Azure OpenAI chat completion service to the kernel\n",
    "// This registers the AI service with the dependency injection container\n",
    "kernelBuilder.AddAzureOpenAIChatCompletion(\n",
    "    model,          // The model name (e.g., \"gpt-4\", \"gpt-35-turbo\")\n",
    "    azureEndpoint,  // Your Azure OpenAI endpoint\n",
    "    apiKey          // Your API key\n",
    ");\n",
    "\n",
    "// Build the kernel instance\n",
    "// This creates a fully configured kernel ready for use\n",
    "Kernel kernel = kernelBuilder.Build();\n",
    "\n",
    "Console.WriteLine(\"Kernel created and configured successfully.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prompt Execution\n",
    "\n",
    "Now let's execute the same query using Semantic Kernel. Notice how much simpler this is compared to the direct API approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel Response:\n",
      "=========================\n",
      "To find the Kubernetes pod that consumes the most resources (CPU and memory), you can use tools like `kubectl` to query the resource usage metrics. This is typically done with the help of Kubernetes' resource metrics API. The `metrics-server` needs to be installed in your cluster to access real-time resource usage data.\n",
      "\n",
      "### Steps to Determine the Most Resource-Consuming Pod\n",
      "\n",
      "#### 1. **Ensure `metrics-server` is Running**\n",
      "The `metrics-server` is a cluster add-on that collects resource metrics like CPU and Memory. You can check if it's running with:\n",
      "\n",
      "```sh\n",
      "kubectl get deployment metrics-server -n kube-system\n",
      "```\n",
      "\n",
      "If it isn't installed, you can deploy it:\n",
      "\n",
      "```sh\n",
      "kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "```\n",
      "\n",
      "#### 2. **Use `kubectl` to Get Current Pod Resource Usage**\n",
      "\n",
      "Run the following `kubectl` command to fetch resource usage for all pods:\n",
      "\n",
      "```sh\n",
      "kubectl top pod --all-namespaces\n",
      "```\n",
      "\n",
      "This will produce output like:\n",
      "\n",
      "```\n",
      "NAMESPACE     NAME                               CPU(cores)   MEMORY(bytes)\n",
      "default       pod-name-1                         100m         200Mi\n",
      "default       pod-name-2                         200m         300Mi\n",
      "kube-system   coredns-xxxxxxxxx-xxxxx            5m           50Mi\n",
      "```\n",
      "\n",
      "You can sort this data manually or copy it to a tool like Excel. However, for automation and accurate sorting, you can use Unix tools or write a custom script.\n",
      "\n",
      "#### 3. **Find the Most Resource-Consuming Pod Automatically**\n",
      "\n",
      "##### CPU Sorting\n",
      "\n",
      "To find the pod consuming the most CPU:\n",
      "\n",
      "```sh\n",
      "kubectl top pod --all-namespaces --no-headers | sort -k3 -nr | head -n 1\n",
      "```\n",
      "\n",
      "**Explanation**:\n",
      "- `--no-headers`: Removes the header from the output.\n",
      "- `sort -k3 -nr`: Sorts by the third column (CPU usage) numerically in descending order.\n",
      "- `head -n 1`: Displays the top result.\n",
      "\n",
      "##### Memory Sorting\n",
      "\n",
      "To find the pod consuming the most memory:\n",
      "\n",
      "```sh\n",
      "kubectl top pod --all-namespaces --no-headers | sort -k4 -nr | head -n 1\n",
      "```\n",
      "\n",
      "**Explanation**:\n",
      "- `sort -k4 -nr`: Sorts by the fourth column (Memory usage) numerically in descending order.\n",
      "\n",
      "#### 4. **Using Custom Scripts**\n",
      "If you want a more specific or detailed report, you can use tools like `awk` or write a custom script in Bash/Python to handle the output parsing.\n",
      "\n",
      "#### 5. **Using Tools like Lens, Prometheus, or Grafana**\n",
      "For a more visual approach, you can use tools such as:\n",
      "- **Lens**: A powerful Kubernetes IDE that provides real-time metrics and insights.\n",
      "- **Prometheus** with **Grafana**: Monitor Kubernetes resource usage over time.\n",
      "\n",
      "These tools provide UI dashboards to analyze which pods consume the most resources.\n",
      "\n",
      "#### Note about Resource Requests/Limits\n",
      "- The above commands show *live resource usage*, not the *resource requests* or *limits* defined in pod specifications.\n",
      "- To inspect resource requests/limits, you can check pod YAML definitions:\n",
      "```sh\n",
      "kubectl get pod <pod-name> -n <namespace> -o yaml | grep -A5 \"resources:\"\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "With this approach, you can quickly identify the pod that's consuming the most CPU or memory in your Kubernetes cluster!\n"
     ]
    }
   ],
   "source": [
    "// Execute a simple prompt using Semantic Kernel\n",
    "// InvokePromptAsync is a high-level method that handles all the complexity\n",
    "var result = await kernel.InvokePromptAsync(\"how do i get the k8s pod that consume the most resources\");\n",
    "\n",
    "Console.WriteLine(\"Semantic Kernel Response:\");\n",
    "Console.WriteLine(\"=========================\");\n",
    "Console.WriteLine(result);\n",
    "\n",
    "// Benefits of using InvokePromptAsync:\n",
    "// 1. Automatic service resolution (no need to manually get the chat service)\n",
    "// 2. Built-in error handling and retries\n",
    "// 3. Logging and telemetry out of the box\n",
    "// 4. Consistent API regardless of the underlying AI service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Chat Completion Services Directly\n",
    "\n",
    "Sometimes you need more control over the AI interaction. Semantic Kernel allows you to access the underlying chat completion service while still benefiting from its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion service type: AzureOpenAIChatCompletionService\n",
      "Service attributes: DeploymentName=gpt-4o\n"
     ]
    }
   ],
   "source": [
    "// Import the chat completion namespace\n",
    "using Microsoft.SemanticKernel.ChatCompletion;\n",
    "\n",
    "// Get the chat completion service from the kernel's dependency injection container\n",
    "// This gives you direct access to the underlying AI service\n",
    "IChatCompletionService completionService = kernel.GetRequiredService<IChatCompletionService>();\n",
    "\n",
    "Console.WriteLine($\"Chat completion service type: {completionService.GetType().Name}\");\n",
    "Console.WriteLine($\"Service attributes: {string.Join(\", \", completionService.Attributes.Select(x => $\"{x.Key}={x.Value}\"))}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Chat Completion Service Response:\n",
      "========================================\n",
      "To identify the Kubernetes pod that consumes the most resources (like CPU or memory), you can follow these steps. You can use tools like `kubectl` and resource monitoring solutions such as `Prometheus`, `Grafana`, or Kubernetes Metrics Server.\n",
      "\n",
      "---\n",
      "\n",
      "### **Using `kubectl` and Metrics Server**\n",
      "The easiest approach is to use the Kubernetes built-in Metrics Server. Ensure it is installed and running in your cluster.\n",
      "\n",
      "#### Step 1: Check if Metrics Server is installed\n",
      "Run:\n",
      "```sh\n",
      "kubectl get pods -n kube-system\n",
      "```\n",
      "Look for a pod named `metrics-server`.\n",
      "\n",
      "If Metrics Server is not installed, you can set it up by following the official instructions: [Metrics Server documentation](https://github.com/kubernetes-sigs/metrics-server).\n",
      "\n",
      "---\n",
      "\n",
      "#### Step 2: View Resource Usage for Pods\n",
      "Once Metrics Server is running, use the following command to list the resource usage of all pods:\n",
      "\n",
      "```sh\n",
      "kubectl top pod --all-namespaces\n",
      "```\n",
      "\n",
      "Example output:\n",
      "```\n",
      "NAMESPACE    NAME                        CPU(cores)   MEMORY(bytes)\n",
      "default      my-app-1234                 100m         128Mi\n",
      "kube-system  kube-dns-765f9c878d-abcde   50m          70Mi\n",
      "default      busy-app-5678               200m         256Mi\n",
      "```\n",
      "\n",
      "This will display CPU and memory usage for each pod across all namespaces. Look at the values under `CPU(cores)` and `MEMORY(bytes)` to determine the pod consuming the most resources.\n",
      "\n",
      "---\n",
      "\n",
      "#### Step 3: Sort and Filter Results\n",
      "If the list is large, you can sort it using utilities like `awk` or `grep`. For example:\n",
      "\n",
      "Sort by memory usage:\n",
      "```sh\n",
      "kubectl top pod --all-namespaces | sort -k 4 -r\n",
      "```\n",
      "\n",
      "Sort by CPU usage:\n",
      "```sh\n",
      "kubectl top pod --all-namespaces | sort -k 3 -r\n",
      "```\n",
      "\n",
      "The pod consuming the most resources should appear at the top of the list.\n",
      "\n",
      "---\n",
      "\n",
      "### **Using Prometheus and Grafana**\n",
      "If your cluster is set up with Prometheus and Grafana, you can use those tools for more advanced monitoring and visualization.\n",
      "\n",
      "#### Steps:\n",
      "1. Access Grafana and use the dashboard that monitors Kubernetes Pods.\n",
      "2. Query Prometheus for metrics like `container_cpu_usage_seconds_total` and `container_memory_usage_bytes`.\n",
      "3. Look for resource usage per pod and sort them to find the top consumer.\n",
      "\n",
      "---\n",
      "\n",
      "### **Using Custom Scripts**\n",
      "If you'd prefer automation or have a complex setup, you can write a script using tools like `kubectl`, Python, or Bash.\n",
      "\n",
      "Example (Bash script to find the pod consuming the most memory):\n",
      "```sh\n",
      "kubectl top pod --all-namespaces | awk 'NR>1 {print $2, $3, $4}' | sort -k3 -rh | head -n 1\n",
      "```\n",
      "\n",
      "This script:\n",
      "- Extracts Pod name, CPU, and Memory columns.\n",
      "- Sorts Pods by memory usage in descending order.\n",
      "- Displays the top result.\n",
      "\n",
      "---\n",
      "\n",
      "### **Troubleshooting**\n",
      "- **No data available:** Ensure Metrics Server is installed and the pods have resource requests/limits defined.\n",
      "- **Accuracy:** Metrics may reflect short-term usage spikes. For real-time, aggregated, and historical data, tools like Prometheus/Grafana are recommended.\n",
      "\n",
      "\n",
      "\n",
      "Response metadata:\n",
      "- Role: Assistant\n",
      "- Model ID: gpt-4o\n",
      "- Content length: 2942 characters\n"
     ]
    }
   ],
   "source": [
    "// Use the chat completion service directly\n",
    "// This gives you more control over the request/response cycle\n",
    "var chatResult = await completionService.GetChatMessageContentAsync(\n",
    "    \"how do i get the k8s pod that consume the most resources\");\n",
    "\n",
    "Console.WriteLine(\"Direct Chat Completion Service Response:\");\n",
    "Console.WriteLine(\"========================================\");\n",
    "Console.WriteLine(chatResult);\n",
    "\n",
    "// You can also access additional properties:\n",
    "Console.WriteLine($\"\\nResponse metadata:\");\n",
    "Console.WriteLine($\"- Role: {chatResult.Role}\");\n",
    "Console.WriteLine($\"- Model ID: {chatResult.ModelId}\");\n",
    "Console.WriteLine($\"- Content length: {chatResult.Content?.Length ?? 0} characters\");\n",
    "\n",
    "// This approach is useful when you need:\n",
    "// 1. Access to response metadata\n",
    "// 2. Custom message formatting\n",
    "// 3. Advanced conversation management\n",
    "// 4. Integration with existing chat systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Prompt Templating\n",
    "\n",
    "One of Semantic Kernel's most powerful features is its prompt templating system. This allows you to create reusable, parameterized prompts that can be dynamically filled with data.\n",
    "\n",
    "### Template Syntax\n",
    "\n",
    "Semantic Kernel uses a simple but powerful templating syntax:\n",
    "- `{{$variableName}}` - Substitutes a variable value\n",
    "- `{{functionName}}` - Calls a function\n",
    "- `{{plugin.functionName}}` - Calls a function from a specific plugin\n",
    "\n",
    "Let's see this in action with a text summarization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created:\n",
      "=======================\n",
      "\n",
      "{{$input}}\n",
      "\n",
      "Give me the TLDR in 5 words.\n",
      "\n",
      "\n",
      "Text to summarize:\n",
      "==================\n",
      "\n",
      "    1) A robot may not injure a human being or, through inaction,\n",
      "    allow a human being to come to harm.\n",
      "\n",
      "    2) A robot must obey orders given it by human beings except where\n",
      "    such orders would conflict with the First Law.\n",
      "\n",
      "    3) A robot must protect its own existence as long as such protection\n",
      "    does not conflict with the First or Second Law.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Define a prompt template with variable substitution\n",
    "// The {{$input}} placeholder will be replaced with actual content\n",
    "string skPrompt = @\"\n",
    "{{$input}}\n",
    "\n",
    "Give me the TLDR in 5 words.\n",
    "\";\n",
    "\n",
    "// Sample text to summarize (Asimov's Three Laws of Robotics)\n",
    "var textToSummarize = @\"\n",
    "    1) A robot may not injure a human being or, through inaction,\n",
    "    allow a human being to come to harm.\n",
    "\n",
    "    2) A robot must obey orders given it by human beings except where\n",
    "    such orders would conflict with the First Law.\n",
    "\n",
    "    3) A robot must protect its own existence as long as such protection\n",
    "    does not conflict with the First or Second Law.\n",
    "\";\n",
    "\n",
    "Console.WriteLine(\"Prompt template created:\");\n",
    "Console.WriteLine(\"=======================\");\n",
    "Console.WriteLine(skPrompt);\n",
    "Console.WriteLine(\"\\nText to summarize:\");\n",
    "Console.WriteLine(\"==================\");\n",
    "Console.WriteLine(textToSummarize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Result:\n",
      "=====================\n",
      "Don't harm, obey, self-preserve.\n"
     ]
    }
   ],
   "source": [
    "// Execute the prompt template with variable substitution\n",
    "// The kernel will replace {{$input}} with the value from the arguments dictionary\n",
    "var result = await kernel.InvokePromptAsync(skPrompt, new() { [\"input\"] = textToSummarize });\n",
    "\n",
    "Console.WriteLine(\"Summarization Result:\");\n",
    "Console.WriteLine(\"=====================\");\n",
    "Console.WriteLine(result);\n",
    "\n",
    "// Benefits of prompt templating:\n",
    "// 1. Reusable prompts - write once, use many times\n",
    "// 2. Dynamic content - inject data at runtime\n",
    "// 3. Maintainable - centralized prompt management\n",
    "// 4. Type-safe - compile-time checking of template variables\n",
    "// 5. Composable - combine multiple templates and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability: Microsoft.Extensions.AI â†” Semantic Kernel\n",
    "\n",
    "One of the great features of the Microsoft AI ecosystem is interoperability. You can easily convert between Microsoft.Extensions.AI interfaces and Semantic Kernel services, allowing you to:\n",
    "\n",
    "- Use existing Microsoft.Extensions.AI code with Semantic Kernel\n",
    "- Leverage Semantic Kernel's advanced features in Microsoft.Extensions.AI applications\n",
    "- Gradually migrate between the two approaches\n",
    "\n",
    "**Important Note**: The conversion methods are currently experimental and marked with `SKEXP0001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interoperability demonstration:\n",
      "===============================\n",
      "Original IChatClient type: OpenAIChatClient\n",
      "Converted to IChatCompletionService: ChatClientChatCompletionService\n",
      "Original IChatCompletionService type: AzureOpenAIChatCompletionService\n",
      "Converted to IChatClient: ChatCompletionServiceChatClient\n"
     ]
    }
   ],
   "source": [
    "// Disable the experimental API warning for demonstration purposes\n",
    "#pragma warning disable SKEXP0001\n",
    "\n",
    "// Convert Microsoft.Extensions.AI IChatClient to Semantic Kernel IChatCompletionService\n",
    "IChatCompletionService chatCompletionServiceFromChatClient = chatClient.AsChatCompletionService();\n",
    "\n",
    "// Convert Semantic Kernel IChatCompletionService to Microsoft.Extensions.AI IChatClient\n",
    "IChatClient chatClientFromChatCompletionService = completionService.AsChatClient();\n",
    "\n",
    "Console.WriteLine(\"Interoperability demonstration:\");\n",
    "Console.WriteLine(\"===============================\");\n",
    "Console.WriteLine($\"Original IChatClient type: {chatClient.GetType().Name}\");\n",
    "Console.WriteLine($\"Converted to IChatCompletionService: {chatCompletionServiceFromChatClient.GetType().Name}\");\n",
    "Console.WriteLine($\"Original IChatCompletionService type: {completionService.GetType().Name}\");\n",
    "Console.WriteLine($\"Converted to IChatClient: {chatClientFromChatCompletionService.GetType().Name}\");\n",
    "\n",
    "// Use cases for interoperability:\n",
    "// 1. Legacy code migration - gradually adopt Semantic Kernel\n",
    "// 2. Mixed architectures - use both frameworks in the same application\n",
    "// 3. Testing - easier to mock and test with familiar interfaces\n",
    "// 4. Third-party integration - work with libraries expecting specific interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Local Models\n",
    "\n",
    "Semantic Kernel isn't limited to cloud-based AI services. You can also use it with local models running on your machine. This is particularly useful for:\n",
    "\n",
    "- **Privacy-sensitive applications** - Keep data on-premises\n",
    "- **Cost optimization** - Avoid per-token charges\n",
    "- **Offline scenarios** - Work without internet connectivity\n",
    "- **Development and testing** - Use local models for development\n",
    "\n",
    "### Example: Using LM Studio with Llama 3.2\n",
    "\n",
    "[LM Studio](https://lmstudio.ai/) is a popular tool for running local language models. It provides an OpenAI-compatible API, making it easy to use with Semantic Kernel.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Install LM Studio\n",
    "2. Download a compatible model (e.g., Llama 3.2)\n",
    "3. Start the local server (typically on `http://127.0.0.1:1234`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model kernel configuration:\n",
      "==================================\n",
      "Model: llama-3.2-3b-instruct\n",
      "Endpoint: http://127.0.0.1:1234/v1\n",
      "Status: Configured (requires LM Studio to be running)\n"
     ]
    }
   ],
   "source": [
    "// Example configuration for local models using LM Studio\n",
    "// Note: This will only work if you have LM Studio running locally\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Kernel = Microsoft.SemanticKernel.Kernel;\n",
    "\n",
    "// Create a new kernel builder for local model\n",
    "IKernelBuilder localKernelBuilder = Kernel.CreateBuilder();\n",
    "\n",
    "// Add OpenAI chat completion pointing to local LM Studio server\n",
    "// LM Studio provides an OpenAI-compatible API endpoint\n",
    "localKernelBuilder.AddOpenAIChatCompletion(\n",
    "    \"llama-3.2-3b-instruct\",           // Model name (as configured in LM Studio)\n",
    "    new Uri(\"http://127.0.0.1:1234/v1\"), // Local LM Studio endpoint\n",
    "    \"\"                                   // No API key needed for local models\n",
    ");\n",
    "\n",
    "// Build the kernel for local use\n",
    "Kernel localKernel = localKernelBuilder.Build();\n",
    "\n",
    "Console.WriteLine(\"Local model kernel configuration:\");\n",
    "Console.WriteLine(\"==================================\");\n",
    "Console.WriteLine(\"Model: llama-3.2-3b-instruct\");\n",
    "Console.WriteLine(\"Endpoint: http://127.0.0.1:1234/v1\");\n",
    "Console.WriteLine(\"Status: Configured (requires LM Studio to be running)\");\n",
    "\n",
    "// Note: The following execution will only work if LM Studio is running\n",
    "// Uncomment the lines below to test with a running local model:\n",
    "\n",
    "/*\n",
    "try\n",
    "{\n",
    "    var localResult = await localKernel.InvokePromptAsync(\"how do i get the k8s pod that consume the most resources\");\n",
    "    Console.WriteLine(\"\\nLocal Model Response:\");\n",
    "    Console.WriteLine(\"=====================\");\n",
    "    Console.WriteLine(localResult);\n",
    "}\n",
    "catch (Exception ex)\n",
    "{\n",
    "    Console.WriteLine($\"\\nLocal model not available: {ex.Message}\");\n",
    "    Console.WriteLine(\"Make sure LM Studio is running with the specified model.\");\n",
    "}\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
