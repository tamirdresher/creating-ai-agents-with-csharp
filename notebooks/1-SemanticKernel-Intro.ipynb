{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Kernel Introduction - Enhanced Edition\n",
    "\n",
    "This notebook provides an introduction to Microsoft Semantic Kernel, a powerful framework for building AI-powered applications. We'll explore the core concepts, compare different approaches, and understand how Semantic Kernel simplifies AI integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "First, let's load our configuration settings. This includes API keys, endpoints, and model configurations that we'll use throughout this notebook.\n",
    "\n",
    "If you hadn't done so already, run the notebook [0-AI-settings](./0-AI-settings.ipynb) which will collect the necessary settings in an interactive way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "- Model: gpt-4o\n",
      "- Endpoint: https://eastus.api.cognitive.microsoft.com/\n",
      "- API Key configured: True\n"
     ]
    }
   ],
   "source": [
    "// Load helper functions and configuration settings\n",
    "// This imports utility functions for loading API keys, endpoints, and other settings from settings.json\n",
    "#!import config/Settings.cs \n",
    "\n",
    "// Load configuration from settings.json file\n",
    "// This returns a tuple with all the necessary configuration values\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId, embeddingEndpoint, embeddingApiKey) = Settings.LoadFromFile();\n",
    "\n",
    "Console.WriteLine($\"Configuration loaded:\");\n",
    "Console.WriteLine($\"- Model: {model}\");\n",
    "Console.WriteLine($\"- Endpoint: {azureEndpoint}\");\n",
    "Console.WriteLine($\"- API Key configured: {!string.IsNullOrEmpty(apiKey)}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Direct AI API Calls with Microsoft.Extensions.AI\n",
    "\n",
    "Before diving into Semantic Kernel, let's see how you would typically make direct calls to AI services using Microsoft.Extensions.AI. This will help us understand what Semantic Kernel abstracts away and why it's valuable.\n",
    "\n",
    "### Microsoft.Extensions.AI Overview\n",
    "\n",
    "Microsoft.Extensions.AI is a unified abstraction layer for AI services that provides:\n",
    "- Consistent interfaces across different AI providers\n",
    "- Built-in logging, telemetry, and caching\n",
    "- Dependency injection integration\n",
    "- Rate limiting and retry policies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAI, 2.5.0-beta.1</span></li><li><span>Azure.Core, 1.49.0</span></li><li><span>Microsoft.Extensions.AI, 9.7.1</span></li><li><span>Microsoft.Extensions.AI.Abstractions, 9.10.0</span></li><li><span>Microsoft.Extensions.AI.OpenAI, 9.7.1-preview.1.25365.4</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft.Extensions.AI packages loaded successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Import required NuGet packages for Microsoft.Extensions.AI\n",
    "#r \"nuget: Azure.Core, 1.49.0\"\n",
    "#r \"nuget: Microsoft.Extensions.AI.Abstractions, 9.10.0.0\"\n",
    "#r \"nuget: Microsoft.Extensions.AI, 9.7.1\"\n",
    "#r \"nuget: Azure.AI.OpenAI, 2.5.0-beta.1\"\n",
    "#r \"nuget: Microsoft.Extensions.AI.OpenAI, 9.7.1-preview.1.25365.4\"\n",
    "\n",
    "\n",
    "Console.WriteLine(\"Microsoft.Extensions.AI packages loaded successfully.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct API Response:\n",
      "====================\n",
      "To identify the Kubernetes pod consuming the most resources (CPU or memory), you can use a combination of `kubectl top pods` and other commands to analyze the resource utilization. Below are some methods to achieve this:\n",
      "\n",
      "### Prerequisites\n",
      "1. Ensure that the `metrics-server` is deployed in your Kubernetes cluster, as it is required to pull resource usage metrics from pods.\n",
      "2. Install `kubectl` CLI and ensure you have access to your Kubernetes cluster.\n",
      "\n",
      "---\n",
      "\n",
      "### Steps to Find the Pod Consuming the Most Resources\n",
      "\n",
      "#### 1. Check Resource Usage Across All Namespaces\n",
      "```bash\n",
      "kubectl top pods --all-namespaces\n",
      "```\n",
      "This will display resource usage for all pods across all namespaces, including CPU and memory consumption.\n",
      "\n",
      "#### 2. Sort by CPU Usage\n",
      "To find the pod consuming the most CPU:\n",
      "```bash\n",
      "kubectl top pods --all-namespaces --sort-by=cpu\n",
      "```\n",
      "\n",
      "#### 3. Sort by Memory Usage\n",
      "To find the pod consuming the most memory:\n",
      "```bash\n",
      "kubectl top pods --all-namespaces --sort-by=memory\n",
      "```\n",
      "\n",
      "#### 4. Check Resource Usage in a Specific Namespace\n",
      "If you are looking for a specific namespace, you can exclude `--all-namespaces` and specify the namespace:\n",
      "```bash\n",
      "kubectl top pods -n <namespace>\n",
      "```\n",
      "\n",
      "#### 5. Export and Find Maximum Usage\n",
      "You can export the resource data and process it using a tool like `awk` or `grep` to pinpoint the pod consuming the most resources. For example:\n",
      "\n",
      "**For CPU:**\n",
      "```bash\n",
      "kubectl top pods --all-namespaces | sort -k3 -nr | head -1\n",
      "```\n",
      "\n",
      "**For Memory:**\n",
      "```bash\n",
      "kubectl top pods --all-namespaces | sort -k4 -nr | head -1\n",
      "```\n",
      "\n",
      "Here, `-k3` sorts by the 3rd column (CPU usage), and `-k4` sorts by the 4th column (Memory usage).\n",
      "\n",
      "---\n",
      "\n",
      "### Example Output\n",
      "```bash\n",
      "NAMESPACE     NAME                                  CPU(cores)   MEMORY(bytes)\n",
      "default       nginx-deployment-6ffbc9f76c-k4jpd    10m          1Mi\n",
      "default       nginx-deployment-6ffbc9f76c-nfkz9    5m           500Mi\n",
      "kube-system   coredns-558bd4d5db-lvcf8             2m           9Mi\n",
      "```\n",
      "\n",
      "After running these commands, you can identify the pod consuming the most resources.\n",
      "\n",
      "---\n",
      "\n",
      "### Monitoring Tools (Optional)\n",
      "If you want more persistent and detailed monitoring for resource usage, consider using Kubernetes monitoring tools like:\n",
      "1. **Prometheus and Grafana** for visualizing metrics.\n",
      "2. **Lens** (a Kubernetes IDE) which shows real-time resource consumption.\n",
      "3. Native Kubernetes Dashboard (if installed).\n",
      "\n",
      "These tools can help you monitor resource consumption over time in a more structured and graphical manner.\n"
     ]
    }
   ],
   "source": [
    "// Create a chat client using Microsoft.Extensions.AI\n",
    "\n",
    "using System.ClientModel;\n",
    "using Microsoft.Extensions.AI; \n",
    "using Azure.AI.OpenAI;\n",
    "\n",
    "IChatClient chatClient = new AzureOpenAIClient(\n",
    "        new Uri(azureEndpoint),           // Azure OpenAI endpoint URL\n",
    "        new ApiKeyCredential(apiKey))     // API key for authentication\n",
    "    .GetChatClient(model)                // Get chat client for specific model\n",
    "    .AsIChatClient();                    // Convert to Microsoft.Extensions.AI interface\n",
    "\n",
    "// Make a direct API call to the AI service\n",
    "// This is a simple request-response pattern\n",
    "ChatResponse response = await chatClient.GetResponseAsync(\n",
    "    new ChatMessage(ChatRole.User, \"how do i get the k8s pod that consume the most resources\"));\n",
    "\n",
    "Console.WriteLine(\"Direct API Response:\");\n",
    "Console.WriteLine(\"====================\");\n",
    "Console.WriteLine(response.Text);\n",
    "\n",
    "// Note: With direct API calls, you need to:\n",
    "// 1. Manage authentication and endpoints manually\n",
    "// 2. Handle conversation history yourself\n",
    "// 3. Implement prompt templating from scratch\n",
    "// 4. Build your own plugin/function calling system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using Semantic Kernel\n",
    "\n",
    "Now let's see how Semantic Kernel simplifies AI integration and provides additional capabilities.\n",
    "\n",
    "### Why Semantic Kernel?\n",
    "\n",
    "While direct API calls work, Semantic Kernel provides several advantages:\n",
    "\n",
    "1. **Higher-level abstractions**: Focus on your business logic, not AI plumbing\n",
    "2. **Built-in prompt templating**: Powerful template system with variable substitution\n",
    "3. **Plugin system**: Easy integration of custom functions and external APIs\n",
    "4. **Planning capabilities**: Automatic orchestration of complex AI workflows\n",
    "5. **Memory management**: Built-in conversation history and context management\n",
    "6. **Enterprise features**: Logging, telemetry, security, and scalability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "language_info": {
     "name": "polyglot-notebook"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.67.1</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel package loaded successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Import Semantic Kernel\n",
    "#r \"nuget: Microsoft.SemanticKernel, 1.67.1\"\n",
    "using Microsoft.SemanticKernel;\n",
    "using Kernel = Microsoft.SemanticKernel.Kernel;  // Alias to avoid conflicts\n",
    "\n",
    "Console.WriteLine(\"Semantic Kernel package loaded successfully.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Configuring a Kernel\n",
    "\n",
    "The `Kernel` is the central orchestrator in Semantic Kernel. Think of it as the \"brain\" that coordinates all AI operations, manages services, and executes prompts and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel created and configured successfully.\r\n"
     ]
    }
   ],
   "source": [
    "// Create a Kernel builder - this is the factory for creating Kernel instances\n",
    "IKernelBuilder kernelBuilder = Kernel.CreateBuilder();\n",
    "\n",
    "// Add Azure OpenAI chat completion service to the kernel\n",
    "// This registers the AI service with the dependency injection container\n",
    "kernelBuilder.AddAzureOpenAIChatCompletion(\n",
    "    model,          // The model name (e.g., \"gpt-4\", \"gpt-35-turbo\")\n",
    "    azureEndpoint,  // Your Azure OpenAI endpoint\n",
    "    apiKey          // Your API key\n",
    ");\n",
    "\n",
    "// Build the kernel instance\n",
    "// This creates a fully configured kernel ready for use\n",
    "Kernel kernel = kernelBuilder.Build();\n",
    "\n",
    "Console.WriteLine(\"Kernel created and configured successfully.\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prompt Execution\n",
    "\n",
    "Now let's execute the same query using Semantic Kernel. Notice how much simpler this is compared to the direct API approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Kernel Response:\n",
      "=========================\n",
      "To identify the Kubernetes pod that is consuming the most resources (CPU and/or memory), you can use commands or tools to extract and analyze the resource consumption data. Here's how you can do that:\n",
      "\n",
      "---\n",
      "\n",
      "## CLI Approach Using `kubectl top`:\n",
      "The `kubectl top` command fetches the resource consumption metrics for nodes or pods. It requires the `metrics-server` to be installed in your Kubernetes cluster.\n",
      "\n",
      "### Steps:\n",
      "1. Make sure the `metrics-server` is deployed in your cluster. If it's not, you can install it using:\n",
      "   ```bash\n",
      "   kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "   ```\n",
      "\n",
      "2. To get resource usage for all pods in a specific namespace:\n",
      "   ```bash\n",
      "   kubectl top pods -n <namespace> --sort-by=<resource>\n",
      "   ```\n",
      "   Replace `<namespace>` with the desired namespace and `<resource>` with either `cpu` or `memory`.\n",
      "\n",
      "   Example:\n",
      "   ```bash\n",
      "   kubectl top pods -n default --sort-by=cpu\n",
      "   ```\n",
      "\n",
      "3. To list all pods across all namespaces:\n",
      "   ```bash\n",
      "   kubectl top pods --all-namespaces --sort-by=cpu\n",
      "   ```\n",
      "   OR\n",
      "   ```bash\n",
      "   kubectl top pods --all-namespaces --sort-by=memory\n",
      "   ```\n",
      "\n",
      "   This will show the resource consumption of all pods and sort them by resource usage.\n",
      "\n",
      "---\n",
      "\n",
      "### Example Output:\n",
      "```bash\n",
      "POD                                CPU(cores)   MEMORY(bytes)   \n",
      "app-pod-1                          1m           100Mi\n",
      "app-pod-2                          50m          180Mi\n",
      "app-pod-3                          250m         512Mi\n",
      "```\n",
      "\n",
      "From this output, the pod with the highest CPU or memory usage can be easily identified as the one consuming the most resources.\n",
      "\n",
      "---\n",
      "\n",
      "## Using Monitoring Tools:\n",
      "Alternatively, you can use monitoring tools that provide richer insights and visualization into resource usage. Some popular options are:\n",
      "1. **Prometheus and Grafana**:\n",
      "   - Prometheus collects metrics, and Grafana visualizes them using dashboards.\n",
      "   - Query Prometheus directly using PromQL to get the CPU/memory usage for pods.\n",
      "2. **Kubernetes Dashboard**: The Kubernetes Web UI provides resource usage per pod.\n",
      "3. **Lens**: A desktop application to monitor and manage Kubernetes clusters visually.\n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like guidance on setting up Prometheus, Grafana, or other tools for monitoring your Kubernetes resources.\n"
     ]
    }
   ],
   "source": [
    "// Execute a simple prompt using Semantic Kernel\n",
    "// InvokePromptAsync is a high-level method that handles all the complexity\n",
    "var result = await kernel.InvokePromptAsync(\"how do i get the k8s pod that consume the most resources\");\n",
    "\n",
    "Console.WriteLine(\"Semantic Kernel Response:\");\n",
    "Console.WriteLine(\"=========================\");\n",
    "Console.WriteLine(result);\n",
    "\n",
    "// Benefits of using InvokePromptAsync:\n",
    "// 1. Automatic service resolution (no need to manually get the chat service)\n",
    "// 2. Built-in error handling and retries\n",
    "// 3. Logging and telemetry out of the box\n",
    "// 4. Consistent API regardless of the underlying AI service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Chat Completion Services Directly\n",
    "\n",
    "Sometimes you need more control over the AI interaction. Semantic Kernel allows you to access the underlying chat completion service while still benefiting from its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat completion service type: AzureOpenAIChatCompletionService\n",
      "Service attributes: DeploymentName=gpt-4o\n"
     ]
    }
   ],
   "source": [
    "// Import the chat completion namespace\n",
    "using Microsoft.SemanticKernel.ChatCompletion;\n",
    "\n",
    "// Get the chat completion service from the kernel's dependency injection container\n",
    "// This gives you direct access to the underlying AI service\n",
    "IChatCompletionService completionService = kernel.GetRequiredService<IChatCompletionService>();\n",
    "\n",
    "Console.WriteLine($\"Chat completion service type: {completionService.GetType().Name}\");\n",
    "Console.WriteLine($\"Service attributes: {string.Join(\", \", completionService.Attributes.Select(x => $\"{x.Key}={x.Value}\"))}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Chat Completion Service Response:\n",
      "========================================\n",
      "To identify the Kubernetes (k8s) pod consuming the most resources (CPU or memory), you can rely on various tools and approaches, such as `kubectl`, Kubernetes metrics server, and monitoring solutions like Prometheus or Grafana. Below are some methods to achieve this:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Using `kubectl` and Metrics Server**\n",
      "\n",
      "To use these commands, ensure the Kubernetes Metrics Server is installed in your cluster.\n",
      "\n",
      "#### **Step 1: Find the Pod Consuming the Most CPU**\n",
      "```bash\n",
      "kubectl top pod --all-namespaces --sort-by=cpu\n",
      "```\n",
      "\n",
      "#### **Step 2: Find the Pod Consuming the Most Memory**\n",
      "```bash\n",
      "kubectl top pod --all-namespaces --sort-by=memory\n",
      "```\n",
      "\n",
      "The `kubectl top pod` command retrieves resource utilization (CPU and memory) metrics for pods in your cluster. Using the `--sort-by` flag, you can sort the results by the resources being consumed.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Using Custom `kubectl` Commands to Filter and Sort**\n",
      "If the `kubectl top pod` output is verbose, you can customize it using shell utilities like `awk` or `sort` to filter and identify the most resource-hungry pod.\n",
      "\n",
      "#### **Find Pod with the Highest CPU Usage**\n",
      "```bash\n",
      "kubectl top pod --all-namespaces | awk '{print $2, $3, $NF}' | sort -k2 -rn | head -n 1\n",
      "```\n",
      "\n",
      "#### **Find Pod with the Highest Memory Usage**\n",
      "```bash\n",
      "kubectl top pod --all-namespaces | awk '{print $2, $4, $NF}' | sort -k2 -rn | head -n 1\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Use Prometheus (if Installed)**\n",
      "Prometheus is a powerful monitoring tool often used with Kubernetes. If you have Prometheus installed, you can query resource usage by pods.\n",
      "\n",
      "#### **Query to Find the Highest CPU Pod**\n",
      "```promql\n",
      "topk(1, sum(rate(container_cpu_usage_seconds_total[5m])) by (pod, namespace))\n",
      "```\n",
      "\n",
      "#### **Query to Find the Highest Memory Pod**\n",
      "```promql\n",
      "topk(1, container_memory_working_set_bytes{container!=\"POD\",container!=\"\"})\n",
      "```\n",
      "\n",
      "You can visualize these queries in the Prometheus UI or integrate them with Grafana for a more user-friendly presentation.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Use Monitoring Tools like Grafana**\n",
      "If you're using a frontend monitoring solution such as Grafana integrated with Prometheus or similar backend tools, you can create dashboards to monitor and rank pods by resource usage. You can use the Prometheus queries shared above to create visualizations.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Additional Notes**\n",
      "- Ensure that the Kubernetes Metrics Server is installed for `kubectl top pod` to work. You can install it using the official [Metrics Server documentation](https://github.com/kubernetes-sigs/metrics-server).\n",
      "- If resource consumption seems excessively high for a pod, you may want to inspect its logs (`kubectl logs`), describe the pod (`kubectl describe pod`), or review its configuration for any anomalies.\n",
      "- Consider setting resource requests and limits in your Kubernetes manifests to prevent runaway resource consumption.\n",
      "\n",
      "Using the above methods, you can quickly identify which Kubernetes pod is consuming the most resources. Let me know if you need further clarification!\n",
      "\n",
      "Response metadata:\n",
      "- Role: Assistant\n",
      "- Model ID: gpt-4o\n",
      "- Content length: 2975 characters\n"
     ]
    }
   ],
   "source": [
    "// Use the chat completion service directly\n",
    "// This gives you more control over the request/response cycle\n",
    "var chatResult = await completionService.GetChatMessageContentAsync(\n",
    "    \"how do i get the k8s pod that consume the most resources\");\n",
    "\n",
    "Console.WriteLine(\"Direct Chat Completion Service Response:\");\n",
    "Console.WriteLine(\"========================================\");\n",
    "Console.WriteLine(chatResult);\n",
    "\n",
    "// You can also access additional properties:\n",
    "Console.WriteLine($\"\\nResponse metadata:\");\n",
    "Console.WriteLine($\"- Role: {chatResult.Role}\");\n",
    "Console.WriteLine($\"- Model ID: {chatResult.ModelId}\");\n",
    "Console.WriteLine($\"- Content length: {chatResult.Content?.Length ?? 0} characters\");\n",
    "\n",
    "// This approach is useful when you need:\n",
    "// 1. Access to response metadata\n",
    "// 2. Custom message formatting\n",
    "// 3. Advanced conversation management\n",
    "// 4. Integration with existing chat systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Prompt Templating\n",
    "\n",
    "One of Semantic Kernel's most powerful features is its prompt templating system. This allows you to create reusable, parameterized prompts that can be dynamically filled with data.\n",
    "\n",
    "### Template Syntax\n",
    "\n",
    "Semantic Kernel uses a simple but powerful templating syntax:\n",
    "- `{{$variableName}}` - Substitutes a variable value\n",
    "- `{{functionName}}` - Calls a function\n",
    "- `{{plugin.functionName}}` - Calls a function from a specific plugin\n",
    "\n",
    "Let's see this in action with a text summarization example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt template created:\n",
      "=======================\n",
      "\n",
      "{{$input}}\n",
      "\n",
      "Give me the TLDR in 5 words.\n",
      "\n",
      "\n",
      "Text to summarize:\n",
      "==================\n",
      "\n",
      "    1) A robot may not injure a human being or, through inaction,\n",
      "    allow a human being to come to harm.\n",
      "\n",
      "    2) A robot must obey orders given it by human beings except where\n",
      "    such orders would conflict with the First Law.\n",
      "\n",
      "    3) A robot must protect its own existence as long as such protection\n",
      "    does not conflict with the First or Second Law.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Define a prompt template with variable substitution\n",
    "// The {{$input}} placeholder will be replaced with actual content\n",
    "string skPrompt = @\"\n",
    "{{$input}}\n",
    "\n",
    "Give me the TLDR in 5 words.\n",
    "\";\n",
    "\n",
    "// Sample text to summarize (Asimov's Three Laws of Robotics)\n",
    "var textToSummarize = @\"\n",
    "    1) A robot may not injure a human being or, through inaction,\n",
    "    allow a human being to come to harm.\n",
    "\n",
    "    2) A robot must obey orders given it by human beings except where\n",
    "    such orders would conflict with the First Law.\n",
    "\n",
    "    3) A robot must protect its own existence as long as such protection\n",
    "    does not conflict with the First or Second Law.\n",
    "\";\n",
    "\n",
    "Console.WriteLine(\"Prompt template created:\");\n",
    "Console.WriteLine(\"=======================\");\n",
    "Console.WriteLine(skPrompt);\n",
    "Console.WriteLine(\"\\nText to summarize:\");\n",
    "Console.WriteLine(\"==================\");\n",
    "Console.WriteLine(textToSummarize);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Result:\n",
      "=====================\n",
      "Don't harm, obey, self-preserve.\n"
     ]
    }
   ],
   "source": [
    "// Execute the prompt template with variable substitution\n",
    "// The kernel will replace {{$input}} with the value from the arguments dictionary\n",
    "var result = await kernel.InvokePromptAsync(skPrompt, new() { [\"input\"] = textToSummarize });\n",
    "\n",
    "Console.WriteLine(\"Summarization Result:\");\n",
    "Console.WriteLine(\"=====================\");\n",
    "Console.WriteLine(result);\n",
    "\n",
    "// Benefits of prompt templating:\n",
    "// 1. Reusable prompts - write once, use many times\n",
    "// 2. Dynamic content - inject data at runtime\n",
    "// 3. Maintainable - centralized prompt management\n",
    "// 4. Type-safe - compile-time checking of template variables\n",
    "// 5. Composable - combine multiple templates and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperability: Microsoft.Extensions.AI â†” Semantic Kernel\n",
    "\n",
    "One of the great features of the Microsoft AI ecosystem is interoperability. You can easily convert between Microsoft.Extensions.AI interfaces and Semantic Kernel services, allowing you to:\n",
    "\n",
    "- Use existing Microsoft.Extensions.AI code with Semantic Kernel\n",
    "- Leverage Semantic Kernel's advanced features in Microsoft.Extensions.AI applications\n",
    "- Gradually migrate between the two approaches\n",
    "\n",
    "**Important Note**: The conversion methods are currently experimental and marked with `SKEXP0001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interoperability demonstration:\n",
      "===============================\n",
      "Original IChatClient type: OpenAIChatClient\n",
      "Converted to IChatCompletionService: ChatClientChatCompletionService\n",
      "Original IChatCompletionService type: AzureOpenAIChatCompletionService\n",
      "Converted to IChatClient: ChatCompletionServiceChatClient\n"
     ]
    }
   ],
   "source": [
    "// Disable the experimental API warning for demonstration purposes\n",
    "#pragma warning disable SKEXP0001\n",
    "\n",
    "// Convert Microsoft.Extensions.AI IChatClient to Semantic Kernel IChatCompletionService\n",
    "IChatCompletionService chatCompletionServiceFromChatClient = chatClient.AsChatCompletionService();\n",
    "\n",
    "// Convert Semantic Kernel IChatCompletionService to Microsoft.Extensions.AI IChatClient\n",
    "IChatClient chatClientFromChatCompletionService = completionService.AsChatClient();\n",
    "\n",
    "Console.WriteLine(\"Interoperability demonstration:\");\n",
    "Console.WriteLine(\"===============================\");\n",
    "Console.WriteLine($\"Original IChatClient type: {chatClient.GetType().Name}\");\n",
    "Console.WriteLine($\"Converted to IChatCompletionService: {chatCompletionServiceFromChatClient.GetType().Name}\");\n",
    "Console.WriteLine($\"Original IChatCompletionService type: {completionService.GetType().Name}\");\n",
    "Console.WriteLine($\"Converted to IChatClient: {chatClientFromChatCompletionService.GetType().Name}\");\n",
    "\n",
    "// Use cases for interoperability:\n",
    "// 1. Legacy code migration - gradually adopt Semantic Kernel\n",
    "// 2. Mixed architectures - use both frameworks in the same application\n",
    "// 3. Testing - easier to mock and test with familiar interfaces\n",
    "// 4. Third-party integration - work with libraries expecting specific interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Local Models\n",
    "\n",
    "Semantic Kernel isn't limited to cloud-based AI services. You can also use it with local models running on your machine. This is particularly useful for:\n",
    "\n",
    "- **Privacy-sensitive applications** - Keep data on-premises\n",
    "- **Cost optimization** - Avoid per-token charges\n",
    "- **Offline scenarios** - Work without internet connectivity\n",
    "- **Development and testing** - Use local models for development\n",
    "\n",
    "### Example: Using LM Studio with Llama 3.2\n",
    "\n",
    "[LM Studio](https://lmstudio.ai/) is a popular tool for running local language models. It provides an OpenAI-compatible API, making it easy to use with Semantic Kernel.\n",
    "\n",
    "**Prerequisites**:\n",
    "1. Install LM Studio\n",
    "2. Download a compatible model (e.g., Llama 3.2)\n",
    "3. Start the local server (typically on `http://127.0.0.1:1234`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model kernel configuration:\n",
      "==================================\n",
      "Model: llama-3.2-3b-instruct\n",
      "Endpoint: http://127.0.0.1:1234/v1\n",
      "Status: Configured (requires LM Studio to be running)\n"
     ]
    }
   ],
   "source": [
    "// Example configuration for local models using LM Studio\n",
    "// Note: This will only work if you have LM Studio running locally\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Kernel = Microsoft.SemanticKernel.Kernel;\n",
    "\n",
    "// Create a new kernel builder for local model\n",
    "IKernelBuilder localKernelBuilder = Kernel.CreateBuilder();\n",
    "\n",
    "// Add OpenAI chat completion pointing to local LM Studio server\n",
    "// LM Studio provides an OpenAI-compatible API endpoint\n",
    "localKernelBuilder.AddOpenAIChatCompletion(\n",
    "    \"llama-3.2-3b-instruct\",           // Model name (as configured in LM Studio)\n",
    "    new Uri(\"http://127.0.0.1:1234/v1\"), // Local LM Studio endpoint\n",
    "    \"\"                                   // No API key needed for local models\n",
    ");\n",
    "\n",
    "// Build the kernel for local use\n",
    "Kernel localKernel = localKernelBuilder.Build();\n",
    "\n",
    "Console.WriteLine(\"Local model kernel configuration:\");\n",
    "Console.WriteLine(\"==================================\");\n",
    "Console.WriteLine(\"Model: llama-3.2-3b-instruct\");\n",
    "Console.WriteLine(\"Endpoint: http://127.0.0.1:1234/v1\");\n",
    "Console.WriteLine(\"Status: Configured (requires LM Studio to be running)\");\n",
    "\n",
    "// Note: The following execution will only work if LM Studio is running\n",
    "// Uncomment the lines below to test with a running local model:\n",
    "\n",
    "/*\n",
    "try\n",
    "{\n",
    "    var localResult = await localKernel.InvokePromptAsync(\"how do i get the k8s pod that consume the most resources\");\n",
    "    Console.WriteLine(\"\\nLocal Model Response:\");\n",
    "    Console.WriteLine(\"=====================\");\n",
    "    Console.WriteLine(localResult);\n",
    "}\n",
    "catch (Exception ex)\n",
    "{\n",
    "    Console.WriteLine($\"\\nLocal model not available: {ex.Message}\");\n",
    "    Console.WriteLine(\"Make sure LM Studio is running with the specified model.\");\n",
    "}\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
